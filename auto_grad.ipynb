{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd2ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82be6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires to calculate the gradient(derivative) of that tensor\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a80af629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3., requires_grad=True), tensor(9., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0013de98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backward navigation allowed only once!\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d9ea6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "\n",
    "z = torch.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "460f42bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3., requires_grad=True),\n",
       " tensor(9., grad_fn=<PowBackward0>),\n",
       " tensor(0.4121, grad_fn=<SinBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8349ee80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.4668)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward()\n",
    "dz_dx = x.grad\n",
    "dz_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ade7fb",
   "metadata": {},
   "source": [
    "##### You can call .backward() only once by default in PyTorch because the computation graph used for the backward pass is freed after the gradients are computed. This is done to save memory during training. The computation graph is dynamic and stored in memory until the .backward() call. Once the backward pass is completed, the graph is discarded unless you explicitly request to retain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d0780ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime error:Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y.backward()\n",
    "    dy_dx = x.grad\n",
    "    dy_dx\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime error:{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98f9ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(prediction, target):\n",
    "    epsilon = 1e-8  # To prevent log(0)\n",
    "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = torch.tensor(6.7)\n",
    "y = torch.tensor(0.0)\n",
    "\n",
    "# the independent variable wrt. which the gradients should be calculated.\n",
    "w = torch.tensor(1.0,requires_grad=True)\n",
    "b = torch.tensor(0.0,requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76ce88e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.7000, grad_fn=<AddBackward0>),\n",
       " tensor(0.9988, grad_fn=<SigmoidBackward0>),\n",
       " tensor(6.7012, grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "\n",
    "z = w*x + b\n",
    "y_pred = torch.sigmoid(z)\n",
    "\n",
    "loss = binary_cross_entropy_loss(y_pred,y)\n",
    "\n",
    "z,y_pred,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5af43228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.6918), tensor(0.9988))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backward pass\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "dl_dw = w.grad\n",
    "dl_db = b.grad\n",
    "\n",
    "dl_dw,dl_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2a713",
   "metadata": {},
   "source": [
    "### Vector input tensors for autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "545054a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.], requires_grad=True),\n",
       " tensor(4.6667, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0],requires_grad=True)\n",
    "\n",
    "y = (x**2).mean()\n",
    "\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54cefb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "dy_dx = x.grad\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30f988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd2c2006",
   "metadata": {},
   "source": [
    "### Clearing grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef28d54",
   "metadata": {},
   "source": [
    "##### If you want to call .backward() more than once (e.g., for gradient accumulation or other tasks), you can use the retain_graph=True argument. This tells PyTorch to keep the computation graph in memory after the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad28579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example tensor\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Example computation\n",
    "y = x ** 2\n",
    "\n",
    "# First backward pass\n",
    "y.backward(retain_graph=True)\n",
    "print(x.grad)  # Output: 4.0\n",
    "\n",
    "# Second backward pass (without retaining the computational graph)\n",
    "y.backward()\n",
    "print(x.grad)  # Output: 8.0 (gradients are accumulated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84108e",
   "metadata": {},
   "source": [
    "##### Gradients Are Accumulated:\n",
    "\n",
    "By default, gradients are added to the .grad attribute. This is why the gradient values in the second call above are cumulative.\n",
    "\n",
    "Use x.grad.zero_() to reset gradients if needed.\n",
    "\n",
    "\n",
    "##### retain_graph=True:\n",
    "\n",
    "Keeping the computation graph can lead to increased memory usage. Use it cautiously to avoid memory leaks or inefficiency.\n",
    "\n",
    "\n",
    "##### Typical Use Case:\n",
    "\n",
    "In most training scenarios, you only call .backward() once per forward pass. Retaining the graph is generally not needed unless for specific tasks like second-order gradients (e.g., using .backward() within a loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532d076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient after first backward pass: tensor(4.)\n",
      "Gradient after second backward pass: tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a tensor with requires_grad=True\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Perform a computation\n",
    "y = x ** 2\n",
    "\n",
    "# First backward pass\n",
    "y.backward(retain_graph=True)\n",
    "print(\"Gradient after first backward pass:\", x.grad)  # Output: 4.0\n",
    "\n",
    "# Reset the gradients\n",
    "x.grad.zero_()\n",
    "\n",
    "# Second backward pass (without retaining the computational graph)\n",
    "y.backward()\n",
    "print(\"Gradient after second backward pass:\", x.grad)  # Output: 4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec8f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f82eccf2",
   "metadata": {},
   "source": [
    "### Disabling gradient tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c76650",
   "metadata": {},
   "source": [
    "##### Disabling gradient tracking in PyTorch is useful when you want to perform operations on tensors without tracking their computation history or consuming memory for gradient calculations. This is commonly used in scenarios like inference, evaluation, or when you need to manipulate tensors without involving gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58c0d5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0,requires_grad=True)\n",
    "y = x**2\n",
    "print(x,y)\n",
    "\n",
    "y.backward()\n",
    "dy_dx = x.grad\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061207c9",
   "metadata": {},
   "source": [
    "##### Method 1: using requires_grad_(False) on the original tensor to not keep track of the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32166eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.), tensor(4.))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(False)\n",
    "y = x**2\n",
    "x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e2084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "# Runtime error because now, there is no gradient tracking for x.So, we can't call backward()\n",
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa61b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d619229e",
   "metadata": {},
   "source": [
    "##### Method 2: Using detach() ,  to create a new tensor(a copy of the original) where gradients aren't tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "91e9fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True) tensor(2.)\n",
      "tensor(8., grad_fn=<PowBackward0>) tensor(8.)\n",
      "tensor(12.)\n",
      "Runtime error because there is no gradient tracking for z\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0,requires_grad=True)\n",
    "\n",
    "z = x.detach()\n",
    "\n",
    "print(x,z)\n",
    "\n",
    "y1 = x**3\n",
    "y2 = z**3\n",
    "\n",
    "print(y1,y2)\n",
    "\n",
    "\n",
    "y1.backward()\n",
    "print(x.grad)\n",
    "\n",
    "try:\n",
    "    y2.backward()\n",
    "    print(z.grad)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime error because there is no gradient tracking for z\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55e2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dc49c88",
   "metadata": {},
   "source": [
    "##### Method 3: Using torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5e70d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True) tensor(4.)\n",
      "Calculated y by applying torch.no_grad(), so there is no tracking of gradient of y wrt. x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0,requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x**2\n",
    "\n",
    "z = torch.log(x)\n",
    "\n",
    "\n",
    "print(x,y)\n",
    "\n",
    "try:\n",
    "    y.backward()\n",
    "    dy_dx = x.grad\n",
    "    print(dy_dx)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Calculated y by applying torch.no_grad(), so there is no tracking of gradient of y wrt. x\")\n",
    "\n",
    "\n",
    "# tracked the gradient of z wrt. x because z was computed without applying torch.no_grad()\n",
    "z.backward()\n",
    "dz_dx = x.grad\n",
    "dz_dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ad426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333eb4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c589c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88c314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c958a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
